{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45eb6680",
   "metadata": {},
   "source": [
    "# Feature Engineering-1\n",
    "Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91db99a",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "59667d2f",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of an observation or attribute value for a particular instance in a dataset. Missing data can occur for many reasons, including data entry errors, data corruption, data loss during transmission, or incomplete surveys.\n",
    "\n",
    "It is essential to handle missing values in a dataset because the presence of missing data can lead to biased or incorrect results in statistical analyses or machine learning models. If the missing data is not handled properly, it may result in lower accuracy or poor performance of models.\n",
    "\n",
    "Some algorithms that are not affected by missing values include Decision Trees, Random Forests, and Naive Bayes. These algorithms can directly handle missing values in the data without requiring any preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6687b1",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b18968ad",
   "metadata": {},
   "source": [
    "There are several techniques used to handle missing data. Some of the most common techniques are:\n",
    "\n",
    "1. Deletion:\n",
    "- Listwise Deletion\n",
    "- Pairwise Deletion\n",
    "2. Imputation:\n",
    "- Mean/Mode/Median Imputation\n",
    "- Forward/Backward Fill Imputation\n",
    "- K-Nearest Neighbor (KNN) Imputation\n",
    "- Regression Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b00241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Deletion: Listwise Deletion\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('stud.csv')\n",
    "df = df.dropna()    # drops all rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2a7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Deletion:\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('stud.csv')\n",
    "df = df.dropna(axis=1, how='any')    # drops all columns with any missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51adb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imputation: Mean/Mode/Median Imputation:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "df = pd.read_csv('stud.csv')\n",
    "imputer = SimpleImputer(strategy='mean')    # other strategies: median, mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e6c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward/Backward Fill Imputation:\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('stud.csv')\n",
    "df.fillna(method='ffill', inplace=True)    # fills missing values with the previous value\n",
    "df.fillna(method='bfill', inplace=True)    # fills missing values with the next value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0153f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor (KNN) Imputation:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "df = pd.read_csv('stud.csv')\n",
    "imputer = KNNImputer(n_neighbors=3)    # number of nearest neighbors to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e5e484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Imputation:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "df = pd.read_csv('stud.csv')\n",
    "imputer = IterativeImputer()    # uses a regression model to impute missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11633ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "471c1b93",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53ed6a",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of classes in a dataset is not equal, meaning that one class has significantly more samples than the others. For example, if a dataset has 100 samples, and 90 belong to class A while only 10 belong to class B, the data is imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can have several negative effects on machine learning models. One of the most significant issues is that the model may learn to predict the majority class much better than the minority class, leading to poor performance on the minority class. In other words, the model will be biased towards the majority class.\n",
    "\n",
    "This can lead to serious consequences in many real-world scenarios, such as medical diagnosis, fraud detection, and anomaly detection, where the minority class may be the one that is actually more important.\n",
    "\n",
    "Additionally, models trained on imbalanced data may have a high overall accuracy but low precision and recall for the minority class, which can be misleading and dangerous in certain applications.\n",
    "\n",
    "Therefore, it is essential to handle imbalanced data appropriately by using techniques such as resampling, modifying loss functions, and adjusting decision thresholds to improve model performance on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc65f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c053fb98",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5585c52",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are techniques used in data preprocessing to address imbalanced data.\n",
    "\n",
    "Up-sampling involves increasing the number of samples in the minority class to balance the dataset with the majority class. This can be done by duplicating the existing samples in the minority class, or by creating synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "Down-sampling involves reducing the number of samples in the majority class to balance the dataset with the minority class. This can be done by randomly selecting a subset of samples from the majority class, or by clustering the majority class samples and selecting representative samples from each cluster.\n",
    "\n",
    "An example of when up-sampling and down-sampling are required is in credit card fraud detection. In this scenario, the number of fraudulent transactions is much lower than the number of legitimate transactions, resulting in imbalanced data. Up-sampling can be used to increase the number of fraudulent transactions to balance the dataset and improve the model's ability to detect fraud. On the other hand, down-sampling can be used if the cost of false positives (flagging legitimate transactions as fraudulent) is high and needs to be minimized. In this case, reducing the number of legitimate transactions can help to balance the dataset and improve the model's precision on detecting fraudulent transactions.\n",
    "\n",
    "In general, the choice between up-sampling and down-sampling depends on the specific problem and the cost of false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1293785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74e051e2",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2893ef",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used in machine learning to artificially increase the size of a dataset by creating new, slightly modified versions of the original data. The goal of data augmentation is to increase the variety and diversity of the data, which can help to improve the performance and robustness of machine learning models.\n",
    "\n",
    "One popular technique for data augmentation is SMOTE (Synthetic Minority Over-sampling Technique), which is used to address imbalanced datasets. SMOTE generates new synthetic samples of the minority class by interpolating between existing minority class samples.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "1. For each sample in the minority class, SMOTE selects one or more nearest neighbors from the minority class.\n",
    "\n",
    "2. It then creates new synthetic samples by interpolating between the selected sample and its nearest neighbors.\n",
    "\n",
    "3. The degree of interpolation is controlled by a parameter called the \"sampling ratio\", which specifies the desired ratio of the minority class to the majority class after oversampling.\n",
    "\n",
    "4. SMOTE repeats this process until the desired sampling ratio is achieved.\n",
    "\n",
    "For example, if the original dataset has 100 samples, with 10 samples in the minority class and 90 samples in the majority class, and the desired sampling ratio is 1:2 (i.e., the minority class should have half as many samples as the majority class), SMOTE would generate 80 synthetic samples of the minority class, resulting in a new dataset with 170 samples (90 majority class samples and 80 minority class samples).\n",
    "\n",
    "SMOTE can be a powerful technique for addressing class imbalance, as it can help to balance the dataset without overfitting to the minority class or underfitting to the majority class. However, it is important to use SMOTE with caution and to evaluate its impact on the overall performance of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe66dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82790c95",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ad631",
   "metadata": {},
   "source": [
    "Outliers are data points that deviate significantly from the rest of the data in a dataset. They can be caused by measurement errors, data entry errors, or other factors that lead to data points that are different from the majority of the data.\n",
    "\n",
    "It is essential to handle outliers in a dataset because they can have a significant impact on statistical analysis and machine learning models.\n",
    "\n",
    "Here are some reasons why handling outliers is important:\n",
    "\n",
    "1. Outliers can skew statistical analysis by affecting measures of central tendency, such as mean and median. This can lead to inaccurate conclusions about the data and may misrepresent the actual distribution of the data.\n",
    "\n",
    "2. Outliers can have a disproportionate impact on machine learning models by influencing the calculation of parameters such as coefficients and weights. This can lead to poor performance on new data and affect the model's ability to generalize.\n",
    "\n",
    "3. Outliers can also introduce noise into the data, making it difficult to detect meaningful patterns and relationships between variables. This can lead to overfitting, where the model fits the noise instead of the underlying structure of the data.\n",
    "\n",
    "To handle outliers, various techniques can be used, such as:\n",
    "\n",
    "1. Removing the outlier data points: This approach involves identifying the outliers and removing them from the dataset. However, this technique should be used with caution as it may lead to loss of important information and affect the representativeness of the dataset.\n",
    "\n",
    "2. Transforming the data: This approach involves applying mathematical transformations to the data, such as logarithmic or power transformations, to reduce the effect of outliers.\n",
    "\n",
    "3. Using robust statistical methods: Robust statistical methods, such as the median or trimmed mean, are less sensitive to outliers and can be used to analyze data in the presence of outliers.\n",
    "\n",
    "In summary, handling outliers is essential for accurate statistical analysis and robust machine learning models. Various techniques can be used to handle outliers, and the choice of technique depends on the specific problem and the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612a809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddfa8087",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb776c4",
   "metadata": {},
   "source": [
    "Handling missing data is a common problem in data analysis, and there are various techniques that can be used to deal with missing data. Here are some of the most commonly used techniques:\n",
    "\n",
    "1. Deletion: This involves removing the rows or columns with missing data. If the amount of missing data is relatively small, deleting the rows or columns with missing data may not significantly impact the overall analysis. However, if a large amount of data is missing, deletion can lead to loss of important information and may affect the representativeness of the dataset.\n",
    "\n",
    "2. Imputation: This involves filling in missing data points with estimates or predictions. Imputation can be done using various techniques, such as mean or median imputation, regression imputation, or k-nearest neighbor imputation. The choice of imputation method depends on the characteristics of the data and the specific problem.\n",
    "\n",
    "3. Ignoring missing data: In some cases, it may be possible to ignore the missing data without affecting the overall analysis. For example, if the missing data is from a variable that is not important for the analysis, ignoring the missing data may be a reasonable approach.\n",
    "\n",
    "4. Multiple imputation: This involves generating multiple imputed datasets and then combining them to obtain the final results. Multiple imputation can help to reduce the bias and uncertainty associated with imputation.\n",
    "\n",
    "5. Using specialized models: Some machine learning models, such as decision trees and random forests, can handle missing data directly. These models can be used to predict the missing values based on the available data.\n",
    "\n",
    "In summary, handling missing data is important for accurate data analysis. The choice of technique depends on the specific problem and the characteristics of the data. It is important to carefully evaluate the impact of missing data on the overall analysis and choose the appropriate technique for handling missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c70e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acee057f",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5362ec54",
   "metadata": {},
   "source": [
    "Determining if the missing data is missing at random or if there is a pattern to the missing data is important for choosing the appropriate strategy for handling the missing data. Here are some strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "1. Visual inspection: One simple way to determine if there is a pattern to the missing data is to plot the data and look for any patterns or trends. For example, if the missing data is clustered around certain values or certain times, this may indicate that there is a pattern to the missing data.\n",
    "\n",
    "2. Correlation analysis: Correlation analysis can be used to identify any relationships between the missing data and other variables in the dataset. If there is a significant correlation between the missing data and another variable, this may indicate that there is a pattern to the missing data.\n",
    "\n",
    "3. Statistical tests: Statistical tests, such as the chi-squared test, can be used to test whether the missing data is missing at random or if there is a pattern to the missing data. If the p-value of the statistical test is less than a predetermined significance level, this indicates that there is a pattern to the missing data.\n",
    "\n",
    "4. Imputation techniques: Imputation techniques can also be used to determine if the missing data is missing at random or if there is a pattern to the missing data. For example, mean or median imputation assumes that the missing data is missing at random, while regression imputation assumes that there is a pattern to the missing data.\n",
    "\n",
    "In summary, determining if the missing data is missing at random or if there is a pattern to the missing data is important for choosing the appropriate strategy for handling the missing data. Visual inspection, correlation analysis, statistical tests, and imputation techniques can all be used to determine if there is a pattern to the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e58cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e26448c9",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a8eda",
   "metadata": {},
   "source": [
    "Dealing with imbalanced datasets, such as in the case of medical diagnosis where the majority of patients do not have the condition of interest, can be challenging. Here are some strategies that can be used to evaluate the performance of machine learning models on imbalanced datasets:\n",
    "\n",
    "1. Accuracy is not a good metric: Accuracy is not a good metric to evaluate the performance of a model on an imbalanced dataset because it can be misleading. For example, a model that always predicts the majority class will have high accuracy, but it will not be useful in identifying the minority class.\n",
    "\n",
    "2. Confusion matrix: A confusion matrix can be used to evaluate the performance of a model on an imbalanced dataset. It provides information about the true positive, true negative, false positive, and false negative rates, which can be used to calculate other performance metrics such as precision, recall, and F1 score.\n",
    "\n",
    "3. ROC curve and AUC: ROC (Receiver Operating Characteristic) curve and AUC (Area Under the Curve) can also be used to evaluate the performance of a model on an imbalanced dataset. The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various probability thresholds. The AUC is a single number that represents the overall performance of the model.\n",
    "\n",
    "4. Resampling techniques: Resampling techniques such as over-sampling and under-sampling can also be used to balance the dataset and improve the performance of the model. For example, SMOTE (Synthetic Minority Over-sampling Technique) can be used to create synthetic samples of the minority class to balance the dataset.\n",
    "\n",
    "5. Cost-sensitive learning: Cost-sensitive learning can be used to assign different costs to misclassifying the minority and majority classes. This can help the model to focus more on correctly identifying the minority class.\n",
    "\n",
    "In summary, evaluating the performance of machine learning models on imbalanced datasets requires careful consideration of the metrics used and the techniques used to balance the dataset. Confusion matrix, ROC curve and AUC, resampling techniques, and cost-sensitive learning can all be used to evaluate the performance of machine learning models on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8e98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbcba337",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b5bd5",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset, such as in the case of customer satisfaction where the bulk of customers report being satisfied, we can employ various techniques, including down-sampling the majority class. Here are some methods that can be used to balance the dataset and down-sample the majority class:\n",
    "\n",
    "1. Random under-sampling: This involves randomly selecting a subset of data from the majority class to match the size of the minority class. This method is simple but can result in loss of information.\n",
    "\n",
    "2. Cluster-based under-sampling: This method involves clustering the majority class and selecting a representative sample from each cluster. This can preserve the information and reduce the data loss compared to random under-sampling.\n",
    "\n",
    "3. Tomek links: Tomek links are pairs of samples from different classes that are close to each other. Removing the majority class sample in a Tomek link can lead to better separation of the classes.\n",
    "\n",
    "4. NearMiss: NearMiss is a family of under-sampling methods that select the samples from the majority class that are closest to the minority class. NearMiss-1 selects the samples from the majority class that have the smallest average distance to the three closest samples from the minority class.\n",
    "\n",
    "5. Edited nearest neighbors: This method involves removing the majority class samples that are misclassified by their nearest neighbors from the minority class.\n",
    "\n",
    "6. SMOTE and its variants: SMOTE (Synthetic Minority Over-sampling Technique) involves creating synthetic samples of the minority class to balance the dataset. Its variants, such as Borderline-SMOTE and ADASYN, can improve the performance further.\n",
    "\n",
    "In summary, there are several methods to balance an unbalanced dataset and down-sample the majority class, including random under-sampling, cluster-based under-sampling, Tomek links, NearMiss, edited nearest neighbors, and SMOTE and its variants. The choice of method depends on the characteristics of the dataset and the performance metrics of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d8bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df7cf101",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a26f35",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset with a low percentage of occurrences of a rare event, we can employ various techniques, including up-sampling the minority class. Here are some methods that can be used to balance the dataset and up-sample the minority class:\n",
    "\n",
    "1. Random over-sampling: This involves randomly replicating samples from the minority class to match the size of the majority class. This method is simple but can result in overfitting and loss of information.\n",
    "\n",
    "2. Synthetic Minority Over-sampling Technique (SMOTE): This method involves creating synthetic samples of the minority class by interpolating between neighboring samples. This can improve the generalization performance compared to random over-sampling.\n",
    "\n",
    "3. Adaptive Synthetic Sampling (ADASYN): This method is an extension of SMOTE that increases the density of synthetic samples in regions where the class imbalance is more severe.\n",
    "\n",
    "4. SMOTE with Tomek links: This method involves combining SMOTE with Tomek links to remove noisy synthetic samples.\n",
    "\n",
    "5. Synthetic Minority Over-sampling TEchnique for Nominal and Continuous (SMOTENC): This method extends SMOTE to handle datasets with a mix of categorical and numerical features.\n",
    "\n",
    "6. Synthetic Data Augmentation (SDA): This method involves using generative models, such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), to generate synthetic samples of the minority class.\n",
    "\n",
    "In summary, there are several methods to balance an unbalanced dataset with a low percentage of occurrences of a rare event, including random over-sampling, SMOTE, ADASYN, SMOTE with Tomek links, SMOTENC, and SDA. The choice of method depends on the characteristics of the dataset and the performance metrics of interest."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e8768bf",
   "metadata": {},
   "source": [
    "Thank You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4d4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
